{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“š Book Fixer Worker\n",
        "\n",
        "This notebook fixes books using free LLM APIs. Run all cells to start processing.\n",
        "\n",
        "**Setup:**\n",
        "1. Set your API keys in the cell below\n",
        "2. Paste the work URL from the coordinator\n",
        "3. Run all cells!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ðŸ”‘ API Keys Configuration\n",
        "#@markdown Get free API keys from:\n",
        "#@markdown - [Groq](https://console.groq.com) - Fast, free tier\n",
        "#@markdown - [Together](https://api.together.xyz) - Free credits\n",
        "#@markdown - [Cerebras](https://cerebras.ai) - Free inference\n",
        "#@markdown - [OpenRouter](https://openrouter.ai) - Free models\n",
        "#@markdown - [DeepSeek](https://deepseek.com) - Very cheap\n",
        "\n",
        "import os\n",
        "\n",
        "GROQ_API_KEY = \"\" #@param {type:\"string\"}\n",
        "TOGETHER_KEY = \"\" #@param {type:\"string\"}\n",
        "CEREBRAS_API_KEY = \"\" #@param {type:\"string\"}\n",
        "OPENROUTER_KEY = \"\" #@param {type:\"string\"}\n",
        "DEEPSEEK_API_KEY = \"\" #@param {type:\"string\"}\n",
        "FIREWORKS_API_KEY = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
        "os.environ['TOGETHER_KEY'] = TOGETHER_KEY\n",
        "os.environ['CEREBRAS_API_KEY'] = CEREBRAS_API_KEY\n",
        "os.environ['OPENROUTER_KEY'] = OPENROUTER_KEY\n",
        "os.environ['DEEPSEEK_API_KEY'] = DEEPSEEK_API_KEY\n",
        "os.environ['FIREWORKS_API_KEY'] = FIREWORKS_API_KEY\n",
        "\n",
        "print(\"âœ… API keys configured!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ðŸ“¥ Work Queue URL\n",
        "#@markdown Paste the work queue URL from the coordinator\n",
        "\n",
        "WORK_URL = \"\" #@param {type:\"string\"}\n",
        "MAX_BOOKS = 10 #@param {type:\"integer\"}\n",
        "\n",
        "print(f\"Will process up to {MAX_BOOKS} books\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ðŸ”§ Worker Code\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "APIS = [\n",
        "    (\"Groq\", \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "     os.environ.get('GROQ_API_KEY', ''), \"llama-3.3-70b-versatile\"),\n",
        "    (\"Cerebras\", \"https://api.cerebras.ai/v1/chat/completions\",\n",
        "     os.environ.get('CEREBRAS_API_KEY', ''), \"llama3.1-70b\"),\n",
        "    (\"Together\", \"https://api.together.xyz/v1/chat/completions\",\n",
        "     os.environ.get('TOGETHER_KEY', ''), \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"),\n",
        "    (\"Fireworks\", \"https://api.fireworks.ai/inference/v1/chat/completions\",\n",
        "     os.environ.get('FIREWORKS_API_KEY', ''), \"accounts/fireworks/models/llama-v3p3-70b-instruct\"),\n",
        "    (\"OpenRouter\", \"https://openrouter.ai/api/v1/chat/completions\",\n",
        "     os.environ.get('OPENROUTER_KEY', ''), \"meta-llama/llama-3.3-70b-instruct\"),\n",
        "    (\"DeepSeek\", \"https://api.deepseek.com/v1/chat/completions\",\n",
        "     os.environ.get('DEEPSEEK_API_KEY', ''), \"deepseek-chat\"),\n",
        "]\n",
        "\n",
        "_api_idx = random.randint(0, len(APIS) - 1)\n",
        "\n",
        "def call_llm(prompt, max_tokens=4000, temperature=0.7):\n",
        "    global _api_idx\n",
        "    for i in range(len(APIS)):\n",
        "        idx = (_api_idx + i) % len(APIS)\n",
        "        name, url, key, model = APIS[idx]\n",
        "        if not key:\n",
        "            continue\n",
        "        try:\n",
        "            headers = {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"}\n",
        "            if \"openrouter\" in url:\n",
        "                headers[\"HTTP-Referer\"] = \"https://colab.research.google.com\"\n",
        "            resp = requests.post(url, headers=headers,\n",
        "                json={\"model\": model, \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                      \"max_tokens\": max_tokens, \"temperature\": temperature},\n",
        "                timeout=120)\n",
        "            if resp.status_code == 200:\n",
        "                print(f\"  âœ“ {name}\")\n",
        "                _api_idx = (idx + 1) % len(APIS)\n",
        "                return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "            elif resp.status_code == 429:\n",
        "                print(f\"  âš  {name} rate limited\")\n",
        "                time.sleep(2)\n",
        "        except Exception as e:\n",
        "            print(f\"  âœ— {name}: {str(e)[:50]}\")\n",
        "    return None\n",
        "\n",
        "def analyze_names(chapters):\n",
        "    all_text = \"\\n\".join(chapters)\n",
        "    prompt = f\"\"\"Analyze this story for character name inconsistencies.\n",
        "Look for same character with different names/spellings.\n",
        "\n",
        "Text (first 8000 chars):\n",
        "{all_text[:8000]}\n",
        "\n",
        "Return JSON array:\n",
        "[{{\"character\": \"main name\", \"variants\": [\"var1\"], \"fix_to\": \"correct\"}}]\n",
        "\n",
        "Return ONLY JSON array.\"\"\"\n",
        "    result = call_llm(prompt, max_tokens=1000)\n",
        "    if result:\n",
        "        try:\n",
        "            start, end = result.find('['), result.rfind(']') + 1\n",
        "            if start >= 0 and end > start:\n",
        "                return json.loads(result[start:end])\n",
        "        except: pass\n",
        "    return []\n",
        "\n",
        "def expand_chapter(text, chapter_num):\n",
        "    word_count = len(text.split())\n",
        "    target = max(3000, word_count + 1000)\n",
        "    prompt = f\"\"\"Expand this chapter to {target} words. Add sensory details, character thoughts, dialogue.\n",
        "Keep same plot and tone.\n",
        "\n",
        "Chapter {chapter_num} ({word_count} words):\n",
        "{text}\n",
        "\n",
        "Write expanded chapter:\"\"\"\n",
        "    result = call_llm(prompt, max_tokens=6000)\n",
        "    return result if result and len(result.split()) > word_count else text\n",
        "\n",
        "def fix_book(book_data):\n",
        "    print(f\"\\n{'='*50}\\nFixing: {book_data['name']}\\n{'='*50}\")\n",
        "    chapters = book_data.get(\"chapters\", {})\n",
        "    if not chapters:\n",
        "        return None\n",
        "    \n",
        "    chapter_list = [chapters[f\"chapter_{i:02d}\"] for i in range(1, 20) if f\"chapter_{i:02d}\" in chapters]\n",
        "    if not chapter_list:\n",
        "        return None\n",
        "    \n",
        "    print(\"\\n[1/3] Analyzing names...\")\n",
        "    name_fixes = analyze_names(chapter_list)\n",
        "    print(f\"  Found {len(name_fixes)} issues\")\n",
        "    \n",
        "    print(\"\\n[2/3] Applying fixes...\")\n",
        "    fixed = {}\n",
        "    for i, text in enumerate(chapter_list, 1):\n",
        "        key = f\"chapter_{i:02d}\"\n",
        "        for fix in name_fixes:\n",
        "            for var in fix.get(\"variants\", []):\n",
        "                text = text.replace(var, fix.get(\"fix_to\", \"\"))\n",
        "        fixed[key] = text\n",
        "    \n",
        "    print(\"\\n[3/3] Expanding short chapters...\")\n",
        "    for key, text in fixed.items():\n",
        "        if len(text.split()) < 2500:\n",
        "            print(f\"  {key}: {len(text.split())} words\")\n",
        "            fixed[key] = expand_chapter(text, key)\n",
        "            print(f\"    â†’ {len(fixed[key].split())} words\")\n",
        "    \n",
        "    return {\"name\": book_data[\"name\"], \"chapters\": fixed, \"fixes\": name_fixes}\n",
        "\n",
        "print(\"âœ… Worker code loaded!\")\n",
        "active = [n for n, _, k, _ in APIS if k]\n",
        "print(f\"Active APIs: {', '.join(active) or 'NONE - set keys above!'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ðŸš€ Run Worker\n",
        "\n",
        "def get_work(url):\n",
        "    if url:\n",
        "        try:\n",
        "            resp = requests.get(url, timeout=30)\n",
        "            if resp.status_code == 200:\n",
        "                return resp.json()\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to get work: {e}\")\n",
        "    \n",
        "    # Demo data if no URL\n",
        "    print(\"Using demo data (no work URL provided)\")\n",
        "    return [{\"name\": \"Demo_Book\", \"chapters\": {\n",
        "        \"chapter_01\": \"John walked in. Jon looked around. Jonn felt scared. The room was dark.\",\n",
        "        \"chapter_02\": \"John found a door. It led somewhere mysterious.\"\n",
        "    }}]\n",
        "\n",
        "def save_results(results):\n",
        "    out = Path(\"fixed_books\")\n",
        "    out.mkdir(exist_ok=True)\n",
        "    for r in results:\n",
        "        if not r: continue\n",
        "        d = out / r[\"name\"]\n",
        "        d.mkdir(exist_ok=True)\n",
        "        for k, v in r[\"chapters\"].items():\n",
        "            (d / f\"{k}.md\").write_text(v)\n",
        "        (d / \"fixes.json\").write_text(json.dumps(r.get(\"fixes\", []), indent=2))\n",
        "        print(f\"Saved: {r['name']}\")\n",
        "\n",
        "# Main execution\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ“š BOOK FIXER WORKER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "books = get_work(WORK_URL)\n",
        "results = []\n",
        "\n",
        "for i, book in enumerate(books[:MAX_BOOKS]):\n",
        "    print(f\"\\n[{i+1}/{min(len(books), MAX_BOOKS)}]\")\n",
        "    result = fix_book(book)\n",
        "    if result:\n",
        "        results.append(result)\n",
        "\n",
        "if results:\n",
        "    save_results(results)\n",
        "    print(f\"\\nâœ… Fixed {len(results)} books!\")\n",
        "    print(\"ðŸ“ Download 'fixed_books' folder from the file browser (left panel)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title ðŸ“¦ Download Results (zip)\n",
        "import shutil\n",
        "\n",
        "if Path(\"fixed_books\").exists():\n",
        "    shutil.make_archive(\"fixed_books\", \"zip\", \"fixed_books\")\n",
        "    print(\"âœ… Created fixed_books.zip\")\n",
        "    print(\"Click the folder icon on the left, then download fixed_books.zip\")\n",
        "    \n",
        "    # For Colab, trigger download\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(\"fixed_books.zip\")\n",
        "    except:\n",
        "        pass\n",
        "else:\n",
        "    print(\"No results to download. Run the worker first!\")"
      ]
    }
  ]
}
