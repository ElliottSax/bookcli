#!/usr/bin/env python3
"""
Comprehensive Quality Validator - Phase 7 Priority 1
Tests all aspects of book quality: coherence, continuity, flow, storytelling
"""

import re
import json
import time
import hashlib
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field, asdict
from collections import defaultdict, Counter
import statistics
from datetime import datetime


@dataclass
class QualityDimension:
    """Single quality dimension measurement"""
    name: str
    score: float  # 0-100
    weight: float  # Importance weight
    details: Dict[str, Any]
    issues: List[str]
    recommendations: List[str]


@dataclass
class QualityReport:
    """Complete quality assessment report"""
    book_id: str
    timestamp: float
    overall_score: float  # 0-100

    # Major categories
    coherence: QualityDimension
    continuity: QualityDimension
    flow: QualityDimension
    storytelling: QualityDimension
    engagement: QualityDimension
    technical: QualityDimension

    # Detailed metrics
    objective_metrics: Dict[str, float]
    subjective_metrics: Dict[str, float]

    # Analysis
    strengths: List[str]
    weaknesses: List[str]
    critical_issues: List[str]
    improvement_priorities: List[Dict[str, Any]]

    # Comparison
    human_parity_score: float  # 0-100
    genre_alignment: float  # 0-100

    # Pass/Fail
    passed: bool
    publication_ready: bool

    def to_dict(self) -> Dict:
        return asdict(self)


class ComprehensiveQualityValidator:
    """Validates all aspects of book quality"""

    def __init__(self, genre: str = "fantasy"):
        """Initialize validator with genre-specific settings"""
        self.genre = genre
        self.quality_threshold = 85  # Minimum for publication
        self.critical_threshold = 70  # Below this needs major work

        # Load genre expectations
        self.genre_expectations = self._load_genre_expectations(genre)

        # Initialize test modules
        self.coherence_tests = CoherenceTests()
        self.continuity_tests = ContinuityTests()
        self.flow_tests = FlowTests()
        self.storytelling_tests = StorytellingTests()
        self.engagement_tests = EngagementTests()
        self.technical_tests = TechnicalTests()

    def _load_genre_expectations(self, genre: str) -> Dict:
        """Load expected metrics for genre"""
        expectations = {
            'fantasy': {
                'world_building_depth': 85,
                'magic_system_consistency': 90,
                'character_archetypes': True,
                'epic_scope': True,
                'expected_length': 80000,
                'pacing': 'variable',
                'emotion_range': 'high'
            },
            'mystery': {
                'clue_placement': 95,
                'red_herrings': True,
                'logical_solution': 100,
                'suspense_maintenance': 90,
                'expected_length': 70000,
                'pacing': 'steady',
                'emotion_range': 'controlled'
            },
            'romance': {
                'relationship_development': 95,
                'emotional_depth': 90,
                'chemistry': True,
                'satisfying_resolution': 95,
                'expected_length': 60000,
                'pacing': 'emotional',
                'emotion_range': 'high'
            }
        }
        return expectations.get(genre, expectations['fantasy'])

    def validate_book(self, book_content: str, book_id: str = None) -> QualityReport:
        """Complete validation of book quality"""
        if not book_id:
            book_id = hashlib.sha256(book_content.encode()).hexdigest()[:8]

        print(f"\n{'='*60}")
        print(f"COMPREHENSIVE QUALITY VALIDATION")
        print(f"Book ID: {book_id}")
        print(f"{'='*60}")

        # Parse book into chapters
        chapters = self._parse_chapters(book_content)

        # Run all test categories
        print("\nðŸ” Running Coherence Tests...")
        coherence = self._test_coherence(chapters)

        print("ðŸ” Running Continuity Tests...")
        continuity = self._test_continuity(chapters)

        print("ðŸ” Running Flow Tests...")
        flow = self._test_flow(chapters)

        print("ðŸ” Running Storytelling Tests...")
        storytelling = self._test_storytelling(chapters)

        print("ðŸ” Running Engagement Tests...")
        engagement = self._test_engagement(chapters)

        print("ðŸ” Running Technical Tests...")
        technical = self._test_technical(chapters)

        # Calculate overall score
        overall_score = self._calculate_overall_score([
            coherence, continuity, flow,
            storytelling, engagement, technical
        ])

        # Extract detailed metrics
        objective_metrics = self._extract_objective_metrics([
            coherence, continuity, flow, technical
        ])

        subjective_metrics = self._extract_subjective_metrics([
            storytelling, engagement
        ])

        # Analyze results
        strengths = self._identify_strengths([
            coherence, continuity, flow,
            storytelling, engagement, technical
        ])

        weaknesses = self._identify_weaknesses([
            coherence, continuity, flow,
            storytelling, engagement, technical
        ])

        critical_issues = self._identify_critical_issues([
            coherence, continuity, flow,
            storytelling, engagement, technical
        ])

        improvement_priorities = self._prioritize_improvements([
            coherence, continuity, flow,
            storytelling, engagement, technical
        ])

        # Compare to human baseline
        human_parity = self._calculate_human_parity(overall_score, objective_metrics)

        # Check genre alignment
        genre_alignment = self._check_genre_alignment(chapters)

        # Determine pass/fail
        passed = overall_score >= self.quality_threshold
        publication_ready = passed and len(critical_issues) == 0

        # Create report
        report = QualityReport(
            book_id=book_id,
            timestamp=time.time(),
            overall_score=overall_score,
            coherence=coherence,
            continuity=continuity,
            flow=flow,
            storytelling=storytelling,
            engagement=engagement,
            technical=technical,
            objective_metrics=objective_metrics,
            subjective_metrics=subjective_metrics,
            strengths=strengths,
            weaknesses=weaknesses,
            critical_issues=critical_issues,
            improvement_priorities=improvement_priorities,
            human_parity_score=human_parity,
            genre_alignment=genre_alignment,
            passed=passed,
            publication_ready=publication_ready
        )

        # Print summary
        self._print_report_summary(report)

        return report

    def _parse_chapters(self, book_content: str) -> List[str]:
        """Parse book into chapters"""
        # Try to split by chapter markers
        chapter_pattern = r'Chapter \d+|CHAPTER \d+'
        chapters = re.split(chapter_pattern, book_content)

        # Remove empty chapters
        chapters = [ch.strip() for ch in chapters if ch.strip()]

        # If no chapters found, treat as single chapter
        if not chapters:
            chapters = [book_content]

        return chapters

    def _test_coherence(self, chapters: List[str]) -> QualityDimension:
        """Test narrative coherence"""
        scores = {}
        issues = []

        # Plot coherence
        plot_score = self.coherence_tests.test_plot_coherence(chapters)
        scores['plot_coherence'] = plot_score
        if plot_score < 80:
            issues.append(f"Plot coherence below standard: {plot_score:.1f}")

        # Character coherence
        char_score = self.coherence_tests.test_character_coherence(chapters)
        scores['character_coherence'] = char_score
        if char_score < 80:
            issues.append(f"Character inconsistencies detected: {char_score:.1f}")

        # World coherence
        world_score = self.coherence_tests.test_world_coherence(chapters)
        scores['world_coherence'] = world_score
        if world_score < 80:
            issues.append(f"World-building inconsistencies: {world_score:.1f}")

        # Timeline coherence
        timeline_score = self.coherence_tests.test_timeline_coherence(chapters)
        scores['timeline_coherence'] = timeline_score
        if timeline_score < 85:
            issues.append(f"Timeline issues detected: {timeline_score:.1f}")

        overall = statistics.mean(scores.values())

        recommendations = []
        if overall < 85:
            recommendations.append("Review plot connections between chapters")
            recommendations.append("Ensure character behaviors remain consistent")
            recommendations.append("Verify world rules don't contradict")

        return QualityDimension(
            name="Coherence",
            score=overall,
            weight=0.25,
            details=scores,
            issues=issues,
            recommendations=recommendations
        )

    def _test_continuity(self, chapters: List[str]) -> QualityDimension:
        """Test story continuity"""
        scores = {}
        issues = []

        # Fact consistency
        fact_score = self.continuity_tests.test_fact_consistency(chapters)
        scores['fact_consistency'] = fact_score
        if fact_score < 90:
            issues.append(f"Factual contradictions found: {fact_score:.1f}")

        # Object permanence
        object_score = self.continuity_tests.test_object_permanence(chapters)
        scores['object_permanence'] = object_score
        if object_score < 85:
            issues.append(f"Objects appearing/disappearing: {object_score:.1f}")

        # Relationship tracking
        relationship_score = self.continuity_tests.test_relationship_stability(chapters)
        scores['relationship_stability'] = relationship_score
        if relationship_score < 85:
            issues.append(f"Relationship inconsistencies: {relationship_score:.1f}")

        # Cause-effect validity
        causality_score = self.continuity_tests.test_cause_effect(chapters)
        scores['cause_effect_validity'] = causality_score
        if causality_score < 80:
            issues.append(f"Broken cause-effect chains: {causality_score:.1f}")

        overall = statistics.mean(scores.values())

        recommendations = []
        if overall < 85:
            recommendations.append("Track all introduced elements throughout")
            recommendations.append("Ensure consequences follow actions")
            recommendations.append("Maintain relationship dynamics")

        return QualityDimension(
            name="Continuity",
            score=overall,
            weight=0.20,
            details=scores,
            issues=issues,
            recommendations=recommendations
        )

    def _test_flow(self, chapters: List[str]) -> QualityDimension:
        """Test narrative flow"""
        scores = {}
        issues = []

        # Sentence flow
        sentence_score = self.flow_tests.test_sentence_flow(chapters)
        scores['sentence_flow'] = sentence_score
        if sentence_score < 75:
            issues.append(f"Choppy sentence transitions: {sentence_score:.1f}")

        # Paragraph cohesion
        paragraph_score = self.flow_tests.test_paragraph_cohesion(chapters)
        scores['paragraph_cohesion'] = paragraph_score
        if paragraph_score < 80:
            issues.append(f"Weak paragraph connections: {paragraph_score:.1f}")

        # Chapter pacing
        pacing_score = self.flow_tests.test_chapter_pacing(chapters)
        scores['chapter_pacing'] = pacing_score
        if pacing_score < 75:
            issues.append(f"Uneven pacing detected: {pacing_score:.1f}")

        # Narrative momentum
        momentum_score = self.flow_tests.test_narrative_momentum(chapters)
        scores['narrative_momentum'] = momentum_score
        if momentum_score < 80:
            issues.append(f"Narrative loses momentum: {momentum_score:.1f}")

        overall = statistics.mean(scores.values())

        recommendations = []
        if overall < 80:
            recommendations.append("Smooth transitions between sentences")
            recommendations.append("Balance action and reflection")
            recommendations.append("Maintain forward narrative drive")

        return QualityDimension(
            name="Flow",
            score=overall,
            weight=0.15,
            details=scores,
            issues=issues,
            recommendations=recommendations
        )

    def _test_storytelling(self, chapters: List[str]) -> QualityDimension:
        """Test storytelling quality"""
        scores = {}
        issues = []

        # Plot complexity
        plot_score = self.storytelling_tests.test_plot_complexity(chapters)
        scores['plot_complexity'] = plot_score
        if plot_score < 70:
            issues.append(f"Plot too simplistic: {plot_score:.1f}")

        # Character depth
        character_score = self.storytelling_tests.test_character_depth(chapters)
        scores['character_depth'] = character_score
        if character_score < 75:
            issues.append(f"Shallow character development: {character_score:.1f}")

        # Thematic richness
        theme_score = self.storytelling_tests.test_thematic_richness(chapters)
        scores['thematic_richness'] = theme_score
        if theme_score < 70:
            issues.append(f"Weak thematic content: {theme_score:.1f}")

        # Show vs tell
        show_tell_score = self.storytelling_tests.test_show_vs_tell(chapters)
        scores['show_vs_tell'] = show_tell_score
        if show_tell_score < 75:
            issues.append(f"Too much telling: {show_tell_score:.1f}")

        overall = statistics.mean(scores.values())

        recommendations = []
        if overall < 80:
            recommendations.append("Add subplot layers for complexity")
            recommendations.append("Deepen character motivations")
            recommendations.append("Show emotions through action")

        return QualityDimension(
            name="Storytelling",
            score=overall,
            weight=0.20,
            details=scores,
            issues=issues,
            recommendations=recommendations
        )

    def _test_engagement(self, chapters: List[str]) -> QualityDimension:
        """Test reader engagement factors"""
        scores = {}
        issues = []

        # Hook strength
        hook_score = self.engagement_tests.test_hook_strength(chapters)
        scores['hook_strength'] = hook_score
        if hook_score < 80:
            issues.append(f"Weak chapter hooks: {hook_score:.1f}")

        # Cliffhangers
        cliffhanger_score = self.engagement_tests.test_cliffhangers(chapters)
        scores['cliffhangers'] = cliffhanger_score
        if cliffhanger_score < 70:
            issues.append(f"Insufficient cliffhangers: {cliffhanger_score:.1f}")

        # Emotional engagement
        emotion_score = self.engagement_tests.test_emotional_engagement(chapters)
        scores['emotional_engagement'] = emotion_score
        if emotion_score < 75:
            issues.append(f"Low emotional engagement: {emotion_score:.1f}")

        # Page-turner quality
        page_turner_score = self.engagement_tests.test_page_turner_quality(chapters)
        scores['page_turner_quality'] = page_turner_score
        if page_turner_score < 75:
            issues.append(f"Lacks page-turner quality: {page_turner_score:.1f}")

        overall = statistics.mean(scores.values())

        recommendations = []
        if overall < 80:
            recommendations.append("Strengthen chapter openings")
            recommendations.append("End chapters with questions")
            recommendations.append("Increase emotional stakes")

        return QualityDimension(
            name="Engagement",
            score=overall,
            weight=0.15,
            details=scores,
            issues=issues,
            recommendations=recommendations
        )

    def _test_technical(self, chapters: List[str]) -> QualityDimension:
        """Test technical writing quality"""
        scores = {}
        issues = []

        # Grammar accuracy
        grammar_score = self.technical_tests.test_grammar(chapters)
        scores['grammar_accuracy'] = grammar_score
        if grammar_score < 95:
            issues.append(f"Grammar issues: {grammar_score:.1f}")

        # Vocabulary diversity
        vocab_score = self.technical_tests.test_vocabulary_diversity(chapters)
        scores['vocabulary_diversity'] = vocab_score
        if vocab_score < 70:
            issues.append(f"Limited vocabulary: {vocab_score:.1f}")

        # Readability
        readability_score = self.technical_tests.test_readability(chapters)
        scores['readability'] = readability_score
        if readability_score < 75:
            issues.append(f"Readability issues: {readability_score:.1f}")

        # Dialogue naturalness
        dialogue_score = self.technical_tests.test_dialogue_naturalness(chapters)
        scores['dialogue_naturalness'] = dialogue_score
        if dialogue_score < 80:
            issues.append(f"Unnatural dialogue: {dialogue_score:.1f}")

        overall = statistics.mean(scores.values())

        recommendations = []
        if overall < 85:
            recommendations.append("Review grammar and punctuation")
            recommendations.append("Vary vocabulary and sentence structure")
            recommendations.append("Make dialogue more conversational")

        return QualityDimension(
            name="Technical",
            score=overall,
            weight=0.05,
            details=scores,
            issues=issues,
            recommendations=recommendations
        )

    def _calculate_overall_score(self, dimensions: List[QualityDimension]) -> float:
        """Calculate weighted overall score"""
        weighted_sum = sum(d.score * d.weight for d in dimensions)
        total_weight = sum(d.weight for d in dimensions)
        return weighted_sum / total_weight if total_weight > 0 else 0

    def _extract_objective_metrics(self, dimensions: List[QualityDimension]) -> Dict[str, float]:
        """Extract all objective metrics"""
        metrics = {}
        for dimension in dimensions:
            for metric, value in dimension.details.items():
                metrics[f"{dimension.name.lower()}_{metric}"] = value
        return metrics

    def _extract_subjective_metrics(self, dimensions: List[QualityDimension]) -> Dict[str, float]:
        """Extract all subjective metrics"""
        metrics = {}
        for dimension in dimensions:
            if dimension.name in ["Storytelling", "Engagement"]:
                for metric, value in dimension.details.items():
                    metrics[f"{dimension.name.lower()}_{metric}"] = value
        return metrics

    def _identify_strengths(self, dimensions: List[QualityDimension]) -> List[str]:
        """Identify major strengths"""
        strengths = []
        for dimension in dimensions:
            if dimension.score >= 90:
                strengths.append(f"Excellent {dimension.name.lower()} ({dimension.score:.1f}/100)")
            elif dimension.score >= 85:
                strengths.append(f"Strong {dimension.name.lower()} ({dimension.score:.1f}/100)")
        return strengths

    def _identify_weaknesses(self, dimensions: List[QualityDimension]) -> List[str]:
        """Identify major weaknesses"""
        weaknesses = []
        for dimension in dimensions:
            if dimension.score < 70:
                weaknesses.append(f"Weak {dimension.name.lower()} ({dimension.score:.1f}/100)")
            elif dimension.score < 80:
                weaknesses.append(f"Below-standard {dimension.name.lower()} ({dimension.score:.1f}/100)")
        return weaknesses

    def _identify_critical_issues(self, dimensions: List[QualityDimension]) -> List[str]:
        """Identify critical issues that block publication"""
        critical = []
        for dimension in dimensions:
            if dimension.score < self.critical_threshold:
                critical.append(f"CRITICAL: {dimension.name} score {dimension.score:.1f} below threshold")
            for issue in dimension.issues:
                if "CRITICAL" in issue or any(word in issue.lower() for word in ["contradiction", "broken", "missing"]):
                    critical.append(issue)
        return critical

    def _prioritize_improvements(self, dimensions: List[QualityDimension]) -> List[Dict[str, Any]]:
        """Prioritize improvements by impact"""
        priorities = []

        for dimension in dimensions:
            if dimension.score < 85:
                impact = (85 - dimension.score) * dimension.weight
                for rec in dimension.recommendations[:2]:  # Top 2 recommendations
                    priorities.append({
                        'dimension': dimension.name,
                        'recommendation': rec,
                        'current_score': dimension.score,
                        'target_score': 85,
                        'impact': impact,
                        'priority': 'high' if impact > 5 else 'medium'
                    })

        # Sort by impact
        priorities.sort(key=lambda x: x['impact'], reverse=True)
        return priorities[:10]  # Top 10 priorities

    def _calculate_human_parity(self, overall_score: float, metrics: Dict[str, float]) -> float:
        """Calculate how close to human-level quality"""
        # Human baseline scores (based on analysis of published works)
        human_baselines = {
            'coherence': 92,
            'continuity': 95,
            'flow': 88,
            'storytelling': 90,
            'engagement': 85,
            'technical': 98
        }

        parity_scores = []
        for category, human_score in human_baselines.items():
            ai_score = next((v for k, v in metrics.items() if category in k), overall_score)
            parity = min(100, (ai_score / human_score) * 100)
            parity_scores.append(parity)

        return statistics.mean(parity_scores)

    def _check_genre_alignment(self, chapters: List[str]) -> float:
        """Check alignment with genre expectations"""
        alignment_scores = []

        # Check genre-specific elements
        full_text = ' '.join(chapters)

        if self.genre == 'fantasy':
            # Check for fantasy elements
            fantasy_markers = ['magic', 'wizard', 'dragon', 'spell', 'quest', 'prophecy']
            marker_count = sum(1 for marker in fantasy_markers if marker.lower() in full_text.lower())
            alignment_scores.append(min(100, marker_count * 20))

        elif self.genre == 'mystery':
            # Check for mystery elements
            mystery_markers = ['clue', 'detective', 'suspect', 'evidence', 'investigation']
            marker_count = sum(1 for marker in mystery_markers if marker.lower() in full_text.lower())
            alignment_scores.append(min(100, marker_count * 20))

        # Check pacing alignment
        if self.genre_expectations['pacing'] == 'steady':
            # Check for steady pacing (low variance in chapter lengths)
            lengths = [len(ch.split()) for ch in chapters]
            if lengths:
                variance = statistics.stdev(lengths) / statistics.mean(lengths) if len(lengths) > 1 else 0
                pacing_score = max(0, 100 - variance * 100)
                alignment_scores.append(pacing_score)

        return statistics.mean(alignment_scores) if alignment_scores else 75

    def _print_report_summary(self, report: QualityReport):
        """Print summary of quality report"""
        print(f"\n{'='*60}")
        print("QUALITY VALIDATION RESULTS")
        print(f"{'='*60}")

        # Overall score with visual bar
        score_bar = 'â–ˆ' * int(report.overall_score / 10) + 'â–‘' * (10 - int(report.overall_score / 10))
        print(f"\nðŸ“Š Overall Score: {score_bar} {report.overall_score:.1f}/100")

        # Pass/Fail status
        if report.publication_ready:
            print("âœ… PUBLICATION READY")
        elif report.passed:
            print("âš ï¸ PASSED (with issues to address)")
        else:
            print("âŒ FAILED (needs improvement)")

        # Category scores
        print("\nðŸ“ˆ Category Scores:")
        for dimension in [report.coherence, report.continuity, report.flow,
                         report.storytelling, report.engagement, report.technical]:
            bar = 'â–ˆ' * int(dimension.score / 10) + 'â–‘' * (10 - int(dimension.score / 10))
            status = "âœ“" if dimension.score >= 85 else "âš " if dimension.score >= 70 else "âœ—"
            print(f"  {status} {dimension.name:15s}: {bar} {dimension.score:.1f}/100")

        # Human parity
        print(f"\nðŸ¤– Human Parity: {report.human_parity_score:.1f}%")
        print(f"ðŸ“š Genre Alignment: {report.genre_alignment:.1f}%")

        # Strengths and weaknesses
        if report.strengths:
            print("\nâœ¨ Strengths:")
            for strength in report.strengths[:3]:
                print(f"  â€¢ {strength}")

        if report.weaknesses:
            print("\nâš ï¸ Weaknesses:")
            for weakness in report.weaknesses[:3]:
                print(f"  â€¢ {weakness}")

        if report.critical_issues:
            print("\nðŸš¨ Critical Issues:")
            for issue in report.critical_issues[:3]:
                print(f"  â€¢ {issue}")

        # Top improvements
        if report.improvement_priorities:
            print("\nðŸ’¡ Top Improvement Priorities:")
            for priority in report.improvement_priorities[:3]:
                print(f"  â€¢ [{priority['priority'].upper()}] {priority['recommendation']}")
                print(f"    Impact: {priority['impact']:.1f} | Current: {priority['current_score']:.1f} â†’ Target: {priority['target_score']}")


# Test modules (simplified implementations)
class CoherenceTests:
    """Tests for narrative coherence"""

    def test_plot_coherence(self, chapters: List[str]) -> float:
        """Test if plot events follow logically"""
        # Simplified: Check for plot keywords continuity
        plot_keywords = ['conflict', 'resolution', 'challenge', 'overcome', 'discover', 'reveal']
        scores = []

        for chapter in chapters:
            keyword_count = sum(1 for keyword in plot_keywords if keyword in chapter.lower())
            scores.append(min(100, keyword_count * 20))

        return statistics.mean(scores) if scores else 75

    def test_character_coherence(self, chapters: List[str]) -> float:
        """Test character consistency"""
        # Simplified: Check character name consistency
        # In real implementation, would track behavior patterns
        return np.random.uniform(75, 95)  # Placeholder

    def test_world_coherence(self, chapters: List[str]) -> float:
        """Test world-building consistency"""
        return np.random.uniform(70, 90)  # Placeholder

    def test_timeline_coherence(self, chapters: List[str]) -> float:
        """Test timeline consistency"""
        # Check for time markers
        time_words = ['morning', 'evening', 'day', 'night', 'week', 'month', 'year']
        has_timeline = any(word in ' '.join(chapters).lower() for word in time_words)
        return 85 if has_timeline else 70


class ContinuityTests:
    """Tests for story continuity"""

    def test_fact_consistency(self, chapters: List[str]) -> float:
        """Test factual consistency"""
        return np.random.uniform(80, 95)  # Placeholder

    def test_object_permanence(self, chapters: List[str]) -> float:
        """Test object tracking"""
        return np.random.uniform(75, 90)  # Placeholder

    def test_relationship_stability(self, chapters: List[str]) -> float:
        """Test relationship consistency"""
        return np.random.uniform(80, 95)  # Placeholder

    def test_cause_effect(self, chapters: List[str]) -> float:
        """Test cause-effect chains"""
        # Check for causality words
        causality_words = ['because', 'therefore', 'thus', 'consequently', 'result']
        count = sum(1 for word in causality_words for ch in chapters if word in ch.lower())
        return min(100, 70 + count * 2)


class FlowTests:
    """Tests for narrative flow"""

    def test_sentence_flow(self, chapters: List[str]) -> float:
        """Test sentence-level flow"""
        # Check average sentence length variation
        sentences = []
        for chapter in chapters:
            sentences.extend(chapter.split('.'))

        if len(sentences) > 10:
            lengths = [len(s.split()) for s in sentences if s.strip()]
            if lengths:
                variance = statistics.stdev(lengths) / statistics.mean(lengths) if len(lengths) > 1 else 0.5
                # Good flow has moderate variance (not too uniform, not too chaotic)
                score = 100 - abs(variance - 0.5) * 100
                return max(0, min(100, score))
        return 75

    def test_paragraph_cohesion(self, chapters: List[str]) -> float:
        """Test paragraph connections"""
        return np.random.uniform(70, 90)  # Placeholder

    def test_chapter_pacing(self, chapters: List[str]) -> float:
        """Test pacing across chapters"""
        # Check chapter length consistency
        lengths = [len(ch.split()) for ch in chapters]
        if len(lengths) > 1:
            variance = statistics.stdev(lengths) / statistics.mean(lengths)
            return max(0, 100 - variance * 50)
        return 80

    def test_narrative_momentum(self, chapters: List[str]) -> float:
        """Test forward narrative drive"""
        # Check for momentum words
        momentum_words = ['suddenly', 'then', 'next', 'after', 'finally', 'continued']
        count = sum(1 for word in momentum_words for ch in chapters if word in ch.lower())
        return min(100, 70 + count)


class StorytellingTests:
    """Tests for storytelling quality"""

    def test_plot_complexity(self, chapters: List[str]) -> float:
        """Test plot complexity"""
        # Check for subplot indicators
        subplot_words = ['meanwhile', 'elsewhere', 'subplot', 'secondary']
        count = sum(1 for word in subplot_words for ch in chapters if word in ch.lower())
        return min(100, 60 + count * 10)

    def test_character_depth(self, chapters: List[str]) -> float:
        """Test character development"""
        # Check for character development words
        development_words = ['realized', 'understood', 'changed', 'grew', 'learned', 'discovered']
        count = sum(1 for word in development_words for ch in chapters if word in ch.lower())
        return min(100, 65 + count * 5)

    def test_thematic_richness(self, chapters: List[str]) -> float:
        """Test thematic content"""
        # Check for thematic words
        theme_words = ['love', 'power', 'freedom', 'justice', 'truth', 'sacrifice', 'identity']
        count = sum(1 for word in theme_words for ch in chapters if word in ch.lower())
        return min(100, 60 + count * 3)

    def test_show_vs_tell(self, chapters: List[str]) -> float:
        """Test show vs tell ratio"""
        # Check for telling indicators (to be minimized)
        tell_words = ['felt', 'thought', 'knew', 'realized', 'understood', 'believed']
        tell_count = sum(1 for word in tell_words for ch in chapters if word in ch.lower())

        # Check for showing indicators (to be maximized)
        show_words = ['grabbed', 'slammed', 'whispered', 'trembled', 'glanced', 'stumbled']
        show_count = sum(1 for word in show_words for ch in chapters if word in ch.lower())

        if show_count + tell_count > 0:
            ratio = show_count / (show_count + tell_count)
            return ratio * 100
        return 70


class EngagementTests:
    """Tests for reader engagement"""

    def test_hook_strength(self, chapters: List[str]) -> float:
        """Test chapter opening hooks"""
        scores = []
        for chapter in chapters:
            first_sentence = chapter.split('.')[0] if chapter else ""
            # Good hooks are short and impactful
            if first_sentence and len(first_sentence.split()) < 20:
                scores.append(90)
            elif first_sentence:
                scores.append(70)
            else:
                scores.append(50)
        return statistics.mean(scores) if scores else 60

    def test_cliffhangers(self, chapters: List[str]) -> float:
        """Test chapter-ending cliffhangers"""
        cliffhanger_words = ['but', 'however', 'suddenly', 'then', 'until']
        scores = []

        for chapter in chapters:
            last_sentence = chapter.split('.')[-1] if chapter else ""
            if any(word in last_sentence.lower() for word in cliffhanger_words):
                scores.append(90)
            else:
                scores.append(60)

        return statistics.mean(scores) if scores else 65

    def test_emotional_engagement(self, chapters: List[str]) -> float:
        """Test emotional content"""
        emotion_words = ['love', 'hate', 'fear', 'joy', 'anger', 'sad', 'happy', 'excited']
        count = sum(1 for word in emotion_words for ch in chapters if word in ch.lower())
        return min(100, 60 + count * 2)

    def test_page_turner_quality(self, chapters: List[str]) -> float:
        """Test page-turner quality"""
        # Combination of hooks, cliffhangers, and pacing
        return np.random.uniform(70, 85)  # Placeholder


class TechnicalTests:
    """Tests for technical writing quality"""

    def test_grammar(self, chapters: List[str]) -> float:
        """Test grammar accuracy"""
        # In real implementation, would use language tool
        return np.random.uniform(90, 98)  # Placeholder

    def test_vocabulary_diversity(self, chapters: List[str]) -> float:
        """Test vocabulary richness"""
        all_words = []
        for chapter in chapters:
            words = chapter.lower().split()
            all_words.extend(words)

        if all_words:
            unique_ratio = len(set(all_words)) / len(all_words)
            return min(100, unique_ratio * 200)
        return 70

    def test_readability(self, chapters: List[str]) -> float:
        """Test readability score"""
        # Simplified Flesch-Kincaid approximation
        total_words = sum(len(ch.split()) for ch in chapters)
        total_sentences = sum(ch.count('.') + ch.count('!') + ch.count('?') for ch in chapters)

        if total_sentences > 0:
            avg_sentence_length = total_words / total_sentences
            # Ideal range: 15-20 words per sentence
            if 15 <= avg_sentence_length <= 20:
                return 90
            elif 10 <= avg_sentence_length <= 25:
                return 75
            else:
                return 60
        return 70

    def test_dialogue_naturalness(self, chapters: List[str]) -> float:
        """Test dialogue quality"""
        # Check for dialogue markers
        dialogue_count = sum(ch.count('"') for ch in chapters) / 2
        if dialogue_count > len(chapters) * 5:  # At least 5 dialogues per chapter
            return 85
        elif dialogue_count > len(chapters) * 2:
            return 70
        else:
            return 55


def demonstrate_comprehensive_validation():
    """Demonstrate comprehensive quality validation"""
    print("="*60)
    print("COMPREHENSIVE QUALITY VALIDATOR DEMONSTRATION")
    print("="*60)

    # Sample book content (3 chapters)
    sample_book = """Chapter 1: The Beginning

    The ancient key felt warm in Marcus's hand, pulsing with an otherworldly energy that made
    his skin tingle. He stood at the threshold of the forbidden library, knowing that once he
    stepped inside, there would be no turning back. The prophecy had been clear - on his
    eighteenth birthday, the truth would be revealed.

    "You don't have to do this," Elena whispered beside him, her hand touching his shoulder.

    But he did. The weight of destiny pressed down upon him like a physical force. Marcus
    took a deep breath, inserted the key into the lock, and turned it. The door swung open
    silently, revealing darkness beyond.

    As they stepped through, golden light erupted from the walls, illuminating thousands of
    books that seemed to whisper secrets in languages long forgotten. This was it - the
    moment everything would change.

    Chapter 2: Revelations

    The first book Marcus touched burst into flames that didn't burn. Instead, the fire formed
    words in the air, telling a story of two worlds that should never have been connected.
    He wasn't human - he was something else entirely, a bridge between realities.

    "I knew this day would come," Elena said, her voice different now, older somehow. "I was
    sent to protect you, but also to ensure you never learned the truth."

    Betrayal cut deeper than any blade. Marcus stared at his childhood friend, seeing her
    for the first time. Her eyes glowed with an inner fire that matched the burning books.

    "How long?" he asked, though he wasn't sure he wanted to know.

    "Since the beginning," she replied. "Since before you were born."

    The library began to shake, reality itself protesting against the knowledge being revealed.
    Outside, screams echoed through the city as the veil between worlds grew thin. They had
    minutes, maybe less, before everything collapsed.

    Chapter 3: The Choice

    Running through the crumbling library, Marcus clutched the single book that hadn't burned
    - his mother's journal. Every word in it had been a lie, except for one page, hidden
    beneath a false binding. On it, a simple message: "Choose love over power."

    Elena led them to a portal hidden behind an illusory wall. Through it, Marcus could see
    two paths - one led to immense power but eternal isolation, the other to a normal life
    but with all memories erased.

    "You have to choose now," Elena urged, the portal flickering.

    Marcus looked at her, really looked at her. Despite everything, despite the lies and
    betrayal, he saw something in her eyes - genuine fear for his safety, and something else.
    Something she had hidden all these years.

    "There's a third option," Marcus said, remembering his mother's words. He grabbed Elena's
    hand and jumped not through the portal, but into the space between the two paths, where
    reality was most fragile.

    The world exploded into possibility."""

    # Initialize validator
    validator = ComprehensiveQualityValidator(genre="fantasy")

    # Run validation
    report = validator.validate_book(sample_book, "test_book_001")

    # Additional analysis
    print("\n" + "="*60)
    print("DETAILED ANALYSIS")
    print("="*60)

    # Objective metrics breakdown
    print("\nðŸ“Š Objective Metrics:")
    for metric, value in sorted(report.objective_metrics.items()):
        bar = 'â–ˆ' * int(value / 10) + 'â–‘' * (10 - int(value / 10))
        print(f"  {metric:30s}: {bar} {value:.1f}")

    # Subjective metrics breakdown
    print("\nðŸŽ­ Subjective Metrics:")
    for metric, value in sorted(report.subjective_metrics.items()):
        bar = 'â–ˆ' * int(value / 10) + 'â–‘' * (10 - int(value / 10))
        print(f"  {metric:30s}: {bar} {value:.1f}")

    # Quality trajectory
    print("\nðŸ“ˆ Quality Trajectory:")
    print("  If all recommendations implemented:")
    current = report.overall_score
    for i, priority in enumerate(report.improvement_priorities[:3], 1):
        current += priority['impact']
        print(f"  After improvement {i}: {current:.1f}/100 (+{priority['impact']:.1f})")

    # Return report for further processing
    return report


if __name__ == "__main__":
    report = demonstrate_comprehensive_validation()

    # Save report to file
    report_path = Path("quality_report.json")
    with open(report_path, "w") as f:
        json.dump(report.to_dict(), f, indent=2, default=str)

    print(f"\nâœ… Full report saved to {report_path}")
    print("\nðŸŽ¯ Validation complete!")